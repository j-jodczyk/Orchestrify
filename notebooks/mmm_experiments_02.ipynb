{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/model.pt\"\n",
    "model = torch.jit.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = model.transformer.wte.weight.shape[1]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miditok # Select a suitable encoding format\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "config = miditok.TokenizerConfig()\n",
    "config.additional_params = { \"base_tokenizer\" : 'MIDILike' }\n",
    "\n",
    "tokenizer = miditok.MMM(config)\n",
    "example_track_path = '../data/external/Jazz Midi/5To10.mid'\n",
    "tokens = tokenizer.encode(example_track_path)\n",
    "# Tokens will now be in a format like a list of integers\n",
    "input_ids = torch.tensor(tokens)  # Convert tokens to tensor for model input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "if len(input_ids) < max_length:\n",
    "    input_ids = torch.cat([input_ids, torch.zeros(max_length - len(input_ids), dtype=torch.long)])\n",
    "else:\n",
    "    input_ids = input_ids[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT2WithClassificationHead(nn.Module):\n",
    "    def __init__(self, model, num_classes):\n",
    "        super(GPT2WithClassificationHead, self).__init__()\n",
    "        self.model = model\n",
    "        # Adding a linear classification layer on top of the model's output\n",
    "        embedding_dim = model.transformer.wte.weight.shape[1]\n",
    "        self.classification_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Pass input through the pre-trained model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the last hidden state for the classification task (last token in each sequence)\n",
    "        last_hidden_state = outputs[0][:, -1, :]  # shape: (batch_size, hidden_dim)\n",
    "        logits = self.classification_head(last_hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Change as needed for your specific task\n",
    "model_with_head = GPT2WithClassificationHead(model, num_classes)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the parameters of the classification head\n",
    "for param in model_with_head.classification_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer to only update the parameters in the classification head\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_with_head.parameters()), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.24.0\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (22.0)\n",
      "Requirement already satisfied: requests in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (4.66.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (1.26.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/julia/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (3.4)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.1\n",
      "    Uninstalling transformers-4.27.1:\n",
      "      Successfully uninstalled transformers-4.27.1\n",
      "Successfully installed transformers-4.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('../local_config.cfg')\n",
    "\n",
    "tokens = dict(config.items('TOKENS'))\n",
    "hf_token = tokens[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9184/2002718638.py:13: UserWarning: miditok - tokenizer.train: `vocab_size` (512) need to be higher than the number of base tokens (529). Skipping tokenizer training.\n",
      "  tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
      "/tmp/ipykernel_9184/2002718638.py:14: UserWarning: miditok: The `save_params` method had been renamed `save`. It is now depreciated and will be removed in future updates.\n",
      "  tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "/tmp/ipykernel_9184/2002718638.py:20: UserWarning: These files have already been split in the saving directory (/home/julia/WIMU/Orchestrify/data/processed). Skipping file splitting.\n",
      "  split_files_for_training(\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer configuration, read the doc to explore other parameters\n",
    "# config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "# tokenizer = REMI(config)\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "midi_paths = list(Path(\"/home/julia/WIMU/Orchestrify/data/external/Jazz Midi\").glob(\"**/*.mid\"))\n",
    "tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
    "tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "tokenizer.push_to_hub(\"juleczka/orchestrify_tokenizer\", private=True, token=hf_token)\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"/home/julia/WIMU/Orchestrify/data/processed\")\n",
    "split_files_for_training(\n",
    "    files_paths=midi_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "forward() is missing value for argument 'argument_2'. Declaration: forward(__torch__.transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel self, Tensor input_ids, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor)) argument_2) -> ((Tensor, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor))))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: forward() is missing value for argument 'argument_2'. Declaration: forward(__torch__.transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel self, Tensor input_ids, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor)) argument_2) -> ((Tensor, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor))))"
     ]
    }
   ],
   "source": [
    "# Set up for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        print(batch.keys())\n",
    "        inputs = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
