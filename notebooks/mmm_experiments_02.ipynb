{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/model.pt\"\n",
    "model = torch.jit.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = model.transformer.wte.weight.shape[1]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miditok # Select a suitable encoding format\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "config = miditok.TokenizerConfig()\n",
    "config.additional_params = { \"base_tokenizer\" : 'MIDILike' }\n",
    "\n",
    "tokenizer = miditok.MMM(config)\n",
    "example_track_path = '../data/external/Jazz Midi/5To10.mid'\n",
    "tokens = tokenizer.encode(example_track_path)\n",
    "# Tokens will now be in a format like a list of integers\n",
    "input_ids = torch.tensor(tokens)  # Convert tokens to tensor for model input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "if len(input_ids) < max_length:\n",
    "    input_ids = torch.cat([input_ids, torch.zeros(max_length - len(input_ids), dtype=torch.long)])\n",
    "else:\n",
    "    input_ids = input_ids[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT2WithClassificationHead(nn.Module):\n",
    "    def __init__(self, model, num_classes):\n",
    "        super(GPT2WithClassificationHead, self).__init__()\n",
    "        self.model = model\n",
    "        # Adding a linear classification layer on top of the model's output\n",
    "        embedding_dim = model.transformer.wte.weight.shape[1]\n",
    "        self.classification_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Pass input through the pre-trained model\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the last hidden state for the classification task (last token in each sequence)\n",
    "        last_hidden_state = outputs[0][:, -1, :]  # shape: (batch_size, hidden_dim)\n",
    "        logits = self.classification_head(last_hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Change as needed for your specific task\n",
    "model_with_head = GPT2WithClassificationHead(model, num_classes)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the parameters of the classification head\n",
    "for param in model_with_head.classification_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer to only update the parameters in the classification head\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_with_head.parameters()), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.24.0\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (22.0)\n",
      "Requirement already satisfied: requests in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (4.66.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from transformers==4.24.0) (1.26.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/julia/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->transformers==4.24.0) (3.4)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.1\n",
      "    Uninstalling transformers-4.27.1:\n",
      "      Successfully uninstalled transformers-4.27.1\n",
      "Successfully installed transformers-4.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('../local_config.cfg')\n",
    "\n",
    "tokens = dict(config.items('TOKENS'))\n",
    "hf_token = tokens[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9184/3223316048.py:13: UserWarning: miditok - tokenizer.train: `vocab_size` (512) need to be higher than the number of base tokens (529). Skipping tokenizer training.\n",
      "  tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
      "/tmp/ipykernel_9184/3223316048.py:14: UserWarning: miditok: The `save_params` method had been renamed `save`. It is now depreciated and will be removed in future updates.\n",
      "  tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Splitting music files (/home/julia/WIMU/Orchestrify/data/processed): 100%|██████████| 934/934 [00:21<00:00, 43.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n",
      "Train your model on this batch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollator)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Iterate over the dataloader to train a model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain your model on this batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/pytorch_data/datasets.py:226\u001b[0m, in \u001b[0;36mDatasetMIDI.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    223\u001b[0m         item[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_key_name] \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n\u001b[0;32m--> 226\u001b[0m tseq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# If not one_token_stream, we only take the first track/sequence\u001b[39;00m\n\u001b[1;32m    228\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m tseq\u001b[38;5;241m.\u001b[39mids \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mone_token_stream \u001b[38;5;28;01melse\u001b[39;00m tseq[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mids\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/pytorch_data/datasets.py:256\u001b[0m, in \u001b[0;36mDatasetMIDI._tokenize_score\u001b[0;34m(self, score)\u001b[0m\n\u001b[1;32m    248\u001b[0m     ac_indexes \u001b[38;5;241m=\u001b[39m create_random_ac_indexes(\n\u001b[1;32m    249\u001b[0m         score,\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mattribute_controls,\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks_idx_random_ratio_range,\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbars_idx_random_ratio_range,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Tokenize it\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m tokseq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_preprocess_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_controls_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# If tokenizing on the fly a multi-stream tokenizer, only keeps the first track\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tokenize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mone_token_stream:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/tokenizations/mmm.py:177\u001b[0m, in \u001b[0;36mMMM.encode\u001b[0;34m(self, score, encode_ids, no_preprocess_score, attribute_controls_indexes, concatenate_track_sequences)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mTokenize a music file (MIDI/abc), given as a ``symusic.Score`` or a file path.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    ``True``, else a list of :class:`miditok.TokSequence` objects.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Need to override to set this class attribute that will be used in\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# `_score_to_tokens`.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_preprocess_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_controls_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Concatenate the sequences\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concatenate_track_sequences:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/midi_tokenizer.py:1658\u001b[0m, in \u001b[0;36mMusicTokenizer.encode\u001b[0;34m(self, score, encode_ids, no_preprocess_score, attribute_controls_indexes)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_score(score)\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;66;03m# Tokenize it\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_controls_indexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;66;03m# Add bar/beat ticks here to TokSeq as they need to be from preprocessed Score\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m add_bar_beats_ticks_to_tokseq(tokens, score)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/midi_tokenizer.py:1179\u001b[0m, in \u001b[0;36mMusicTokenizer._score_to_tokens\u001b[0;34m(self, score, attribute_controls_indexes)\u001b[0m\n\u001b[1;32m   1177\u001b[0m ticks_beats \u001b[38;5;241m=\u001b[39m get_beats_ticks(score, only_notes_onsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ti, track \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(score\u001b[38;5;241m.\u001b[39mtracks):\n\u001b[0;32m-> 1179\u001b[0m     track_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_track_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mticks_per_beat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mticks_per_quarter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mticks_bars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mticks_beats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_controls_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mone_token_stream_for_programs:\n\u001b[1;32m   1188\u001b[0m         all_events \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m track_events\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/midi_tokenizer.py:1380\u001b[0m, in \u001b[0;36mMusicTokenizer._create_track_events\u001b[0;34m(self, track, ticks_per_beat, time_division, ticks_bars, ticks_beats, attribute_controls_indexes)\u001b[0m\n\u001b[1;32m   1377\u001b[0m tpb_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m note \u001b[38;5;129;01min\u001b[39;00m track\u001b[38;5;241m.\u001b[39mnotes:\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# Program\u001b[39;00m\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_programs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprogram_changes:\n\u001b[1;32m   1381\u001b[0m         events\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1382\u001b[0m             Event(\n\u001b[1;32m   1383\u001b[0m                 type_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgram\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1388\u001b[0m             )\n\u001b[1;32m   1389\u001b[0m         )\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;66;03m# Pitch interval\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer configuration, read the doc to explore other parameters\n",
    "# config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "# tokenizer = REMI(config)\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "midi_paths = list(Path(\"/home/julia/WIMU/Orchestrify/data/external/Jazz Midi\").glob(\"**/*.mid\"))\n",
    "tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
    "tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "tokenizer.push_to_hub(\"juleczka/orchestrify_tokenizer\", private=True, token=hf_token)\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"/home/julia/WIMU/Orchestrify/data/processed\")\n",
    "split_files_for_training(\n",
    "    files_paths=midi_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collator)\n",
    "\n",
    "# Iterate over the dataloader to train a model\n",
    "for batch in dataloader:\n",
    "    print(\"Train your model on this batch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Addition to a `TokSequence` object can only be performed with other`TokSequence` objects. Received: list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 32\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m         labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# In language modeling, inputs are labels shifted by one\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[82], line 15\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n\u001b[1;32m     14\u001b[0m tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))  \u001b[38;5;66;03m# Pad to max length\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/miditok/classes.py:291\u001b[0m, in \u001b[0;36mTokSequence.__iadd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, TokSequence):\n\u001b[1;32m    287\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddition to a `TokSequence` object can only be performed with other\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`TokSequence` objects. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m attributes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ids_decoded\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes:\n",
      "\u001b[0;31mValueError\u001b[0m: Addition to a `TokSequence` object can only be performed with other`TokSequence` objects. Received: list"
     ]
    }
   ],
   "source": [
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, midi_files, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.midi_files = midi_files\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        midi = self.midi_files[idx]\n",
    "        tokens = self.tokenizer.encode(midi)\n",
    "        tokens = tokens[:self.max_length]\n",
    "        tokens += [0] * (self.max_length - len(tokens))  # Pad to max length\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "# Example usage\n",
    "midi_files = ['../data/external/Jazz Midi/914.mid', '../data/external/Jazz Midi/ABC.mid']  # List of MIDI file paths\n",
    "dataset = MIDIDataset(midi_files, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Set up for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch.to(device)\n",
    "        labels = inputs.clone()  # In language modeling, inputs are labels shifted by one\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
