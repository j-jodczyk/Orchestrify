{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MODEL_PATH = \"../models/model.pt\"\n",
    "model = torch.jit.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('../local_config.cfg')\n",
    "\n",
    "tokens = dict(config.items('TOKENS'))\n",
    "hf_token = tokens[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3773/233060687.py:18: UserWarning: miditok - tokenizer.train: `vocab_size` (512) need to be higher than the number of base tokens (529). Skipping tokenizer training.\n",
      "  tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
      "/tmp/ipykernel_3773/233060687.py:19: UserWarning: miditok: The `save_params` method had been renamed `save`. It is now depreciated and will be removed in future updates.\n",
      "  tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Splitting music files (/home/julia/WIMU/Orchestrify/data/processed): 100%|██████████| 934/934 [00:13<00:00, 68.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import miditok\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating a multitrack tokenizer configuration, read the doc to explore other parameters\n",
    "# config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "# tokenizer = REMI(config)\n",
    "\n",
    "config = miditok.TokenizerConfig()\n",
    "config.additional_params = { \"base_tokenizer\" : 'MIDILike' }\n",
    "\n",
    "tokenizer = miditok.MMM(config)\n",
    "\n",
    "# Train the tokenizer with Byte Pair Encoding (BPE)\n",
    "midi_paths = list(Path(\"/home/julia/WIMU/Orchestrify/data/external/Jazz Midi\").glob(\"**/*.mid\"))\n",
    "tokenizer.train(vocab_size=512, files_paths=midi_paths)\n",
    "tokenizer.save_params(Path(\"models\", \"tokenizer.json\"))\n",
    "# And pushing it to the Hugging Face hub (you can download it back with .from_pretrained)\n",
    "tokenizer.push_to_hub(\"juleczka/orchestrify_tokenizer\", private=True, token=hf_token)\n",
    "\n",
    "# Split MIDIs into smaller chunks for training\n",
    "dataset_chunks_dir = Path(\"/home/julia/WIMU/Orchestrify/data/processed\")\n",
    "split_files_for_training(\n",
    "    files_paths=midi_paths,\n",
    "    tokenizer=tokenizer,\n",
    "    save_dir=dataset_chunks_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "\n",
    "# Create a Dataset, a DataLoader and a collator to train a model\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(dataset_chunks_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id, copy_inputs_as_labels=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /home/julia/anaconda3/lib/python3.10/site-packages (9.3.0)\n",
      "Requirement already satisfied: webcolors>=1.5 in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (24.11.1)\n",
      "Requirement already satisfied: joblib in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (3.7.0)\n",
      "Requirement already satisfied: numpy in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (1.26.4)\n",
      "Requirement already satisfied: chardet in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: more-itertools in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (10.5.0)\n",
      "Requirement already satisfied: jsonpickle in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (4.0.0)\n",
      "Requirement already satisfied: requests in /home/julia/anaconda3/lib/python3.10/site-packages (from music21) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/julia/anaconda3/lib/python3.10/site-packages (from matplotlib->music21) (9.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->music21) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->music21) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->music21) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/julia/anaconda3/lib/python3.10/site-packages (from requests->music21) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/julia/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasetcreatorconfig\n",
    "import datasetcreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creator_config = datasetcreatorconfig.JSBDatasetCreatorTrackConfig()\n",
    "dataset_creator = datasetcreator.DatasetCreator(dataset_creator_config)\n",
    "dataset_creator.create(datasets_path='../data/external/Jazz Midi', overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TokenSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset_paths, block_size, simulate=False):\n",
    "\n",
    "        pad_token_id = tokenizer.encode(\"[PAD]\")[0]\n",
    "        unk_token_id = tokenizer.encode(\"[UNK]\")[0]\n",
    "\n",
    "        # Read all lines from all files.\n",
    "        lines = []\n",
    "        for dataset_path in dataset_paths:\n",
    "            assert os.path.isfile(dataset_path), f\"Input file path {dataset_path} not found\"\n",
    "            lines += open(dataset_path, \"r\").readlines()\n",
    "\n",
    "        # In simulation just use a few samples.\n",
    "        if simulate:\n",
    "            random.shuffle(lines)\n",
    "            lines = lines[:10]\n",
    "\n",
    "        # Turn lines into training examples. Also gather some statistics.\n",
    "        self.examples = []\n",
    "        unknown_tokens_set = []\n",
    "        unknown_tokens = []\n",
    "        tokens_count = 0\n",
    "        unknown_token_lines_count = 0\n",
    "        too_long_lines_count = 0\n",
    "        encoded_lengths = []\n",
    "        for line in lines:\n",
    "\n",
    "            #Skip empty lines.\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue\n",
    "\n",
    "            # Encode the line.\n",
    "            encoded_line = tokenizer.encode(line)\n",
    "            encoded_lengths += [len(encoded_line)]\n",
    "            tokens_count += len(encoded_line)\n",
    "\n",
    "            # Create a warning about unknown tokens. And then skip the line.\n",
    "            if unk_token_id in encoded_line:\n",
    "                index = encoded_line.index(unk_token_id)\n",
    "                token = tokenizer.decode(encoded_line[index])\n",
    "                token = line.split()[index]\n",
    "                if token not in unknown_tokens_set:\n",
    "                    unknown_tokens_set += [token]\n",
    "                #logger.warning(f\"Skipping line because of unknown token {token}\")\n",
    "                unknown_tokens += [token]\n",
    "                unknown_token_lines_count += 1\n",
    "                continue\n",
    "\n",
    "            # Skip sequence if it is too long.\n",
    "            if len(encoded_line) > block_size:\n",
    "                #logger.warning(f\"Skipping line because it is too long... {len(encoded_line)} > {block_size}\")\n",
    "                too_long_lines_count += 1\n",
    "                continue\n",
    "\n",
    "            # Pad and truncate.\n",
    "            tensor = np.full((block_size,), pad_token_id, dtype=np.longlong)\n",
    "            tensor[:len(encoded_line)] = encoded_line\n",
    "            assert len(tensor) == block_size\n",
    "\n",
    "            self.examples += [{\n",
    "                \"input_ids\": torch.tensor(tensor, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(tensor, dtype=torch.long)\n",
    "            }]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = './models_2'\n",
    "\n",
    "# tokenizer = Tokenizer.from_file('../data/external/Jazz Midi/jsb_mmmtrack/tokenizer.json')\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='../data/external/Jazz Midi/jsb_mmmtrack/tokenizer.json')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_path),\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_dir=os.path.join(output_path, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=768\n",
    ")\n",
    "\n",
    "dataset_train = TokenSequenceDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_paths=['../data/external/Jazz Midi/jsb_mmmtrack/token_sequences_train.txt'],\n",
    "    block_size=768,\n",
    "    simulate=False\n",
    ")\n",
    "\n",
    "dataset_valid = TokenSequenceDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_paths=['../data/external/Jazz Midi/jsb_mmmtrack/token_sequences_valid.txt'],\n",
    "    block_size=768,\n",
    "    simulate=False\n",
    ")\n",
    "\n",
    "# def patched_get(self, obj, cls):\n",
    "#     return self.__getattribute__(\"forward\")\n",
    "# torch.jit._script._CachedForward.__get__ = patched_get\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=dataset_train,\n",
    "#     eval_dataset=dataset_valid\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset_valid, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input_ids: Tensor,\n",
      "    argument_2: Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]]:\n",
      "  lm_head = self.lm_head\n",
      "  transformer = self.transformer\n",
      "  _0, _1, _2, _3, _4, _5, = argument_2\n",
      "  _6, past_value, = _0\n",
      "  _7, past_value0, = _1\n",
      "  _8, past_value1, = _2\n",
      "  _9, past_value2, = _3\n",
      "  _10, past_value3, = _4\n",
      "  _11, past_value4, = _5\n",
      "  _12 = (transformer).forward(input_ids, _6, past_value, _7, past_value0, _8, past_value1, _9, past_value2, _10, past_value3, _11, past_value4, )\n",
      "  _13, _14, _15, _16, _17, _18, _19, _20, _21, _22, _23, _24, _25, = _12\n",
      "  _26 = (lm_head).forward(_13, )\n",
      "  _27 = ((_14, _15), (_16, _17), (_18, _19), (_20, _21), (_22, _23), (_24, _25))\n",
      "  return (_26, _27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize `argument_2` with empty tensors\n",
    "# The dimensions and data types here are based on your model's expected format\n",
    "batch = next(iter(dataloader))\n",
    "input_ids = batch['input_ids']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = input_ids.size(0)\n",
    "num_heads = 8           # Set based on model's actual configuration\n",
    "hidden_dim = 512         # This is usually hidden_size / num_heads\n",
    "sequence_length = input_ids.size(1)\n",
    "\n",
    "argument_2 = tuple(\n",
    "    (\n",
    "        torch.zeros((batch_size, num_heads, sequence_length, hidden_dim // num_heads), dtype=torch.float32).to(device),  # past_key\n",
    "        torch.zeros((batch_size, num_heads, sequence_length, hidden_dim // num_heads), dtype=torch.float32).to(device)   # past_value\n",
    "    )\n",
    "    for _ in range(6)  # Six layers, assuming GPT-2 small\n",
    ")\n",
    "\n",
    "# Generation loop\n",
    "generated_sequence = []\n",
    "\n",
    "# Define generation parameters\n",
    "max_length = 50  # Adjust based on desired output length\n",
    "end_token_id = 1  # Replace with the correct end token ID\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        print(_)\n",
    "        # Pass the current input and past states to the model\n",
    "        output_logits, argument_2 = model(input_ids, argument_2)\n",
    "\n",
    "        # Get the next token ID (e.g., using argmax for simplicity; replace with sampling if needed)\n",
    "        next_token_id = torch.argmax(output_logits, dim=-1)[0, -1]  # Last token's prediction\n",
    "\n",
    "        # Append to the generated sequence\n",
    "        generated_sequence.append(next_token_id.item())\n",
    "\n",
    "        # Stop if the end token is generated\n",
    "        if next_token_id.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids for the next iteration\n",
    "        probs = torch.softmax(output_logits, dim=-1)\n",
    "        # Sample the next token from the probability distribution\n",
    "        next_token_id = torch.multinomial(probs[0, -1], num_samples=1)# Make it batch-size compatible\n",
    "\n",
    "# Print the generated token IDs\n",
    "print(\"Generated sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "forward() Expected a value of type 'Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]' for argument 'argument_2' but instead found type 'Tensor'.\nPosition: 2\nValue: tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]])\nDeclaration: forward(__torch__.transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel self, Tensor input_ids, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor)) argument_2) -> ((Tensor, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor))))\nCast error details: Object tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]) had a different number of elements than type Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m attention \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: forward() Expected a value of type 'Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]' for argument 'argument_2' but instead found type 'Tensor'.\nPosition: 2\nValue: tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]])\nDeclaration: forward(__torch__.transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel self, Tensor input_ids, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor)) argument_2) -> ((Tensor, ((Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor), (Tensor, Tensor))))\nCast error details: Object tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]) had a different number of elements than type Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]"
     ]
    }
   ],
   "source": [
    "# Set up for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_tracks = 6  # From the Tuple structure of argument_2\n",
    "hidden_size = 768  # Replace with your model's hidden state size\n",
    "sequence_length = 1  # Start with a single token\n",
    "\n",
    "# Create empty past state tensors\n",
    "empty_tensor = torch.zeros((1, sequence_length, hidden_size), dtype=torch.float32)\n",
    "argument_2 = tuple((empty_tensor.clone(), empty_tensor.clone()) for _ in range(num_tracks))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        attention = batch['attention_mask']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, argument_2 = model(inputs, argument_2)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        next_token_id = torch.argmax(output_logits, dim=-1)[:, -1]\n",
    "        generated_sequence.append(next_token_id.item())\n",
    "\n",
    "        if next_token_id.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "        input_ids = next_token_id.unsqueeze(0)\n",
    "\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
